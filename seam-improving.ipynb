{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10123913,"sourceType":"datasetVersion","datasetId":6247281}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install petroscope","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:43:31.470622Z","iopub.execute_input":"2025-02-01T20:43:31.470838Z","iopub.status.idle":"2025-02-01T20:43:31.474441Z","shell.execute_reply.started":"2025-02-01T20:43:31.470817Z","shell.execute_reply":"2025-02-01T20:43:31.473570Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import gc\nfrom pathlib import Path\nfrom time import time\nfrom typing import Iterable, List, Tuple, Union\n\nimport cv2\nimport kornia.feature as KF\nimport numpy as np\nimport torch\nimport torchvision\nfrom PIL import Image\nfrom scipy.optimize import least_squares\n\n\n\n# from petroscope.panoramas.utils import transform_and_stitch, optimize","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:44:51.679513Z","iopub.execute_input":"2025-02-01T20:44:51.679878Z","iopub.status.idle":"2025-02-01T20:44:51.832234Z","shell.execute_reply.started":"2025-02-01T20:44:51.679851Z","shell.execute_reply":"2025-02-01T20:44:51.831601Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def transform_and_stitch(\n    transforms: List[np.ndarray], img_paths: Iterable[Path]\n) -> np.ndarray:\n    \"\"\"Stitches all the images into a single panorama\n    using known transformations.\n\n    Args:\n        transforms : List of 3x3 homography matrices.\n        img_paths : List of Path objects for each image of the panorama.\n\n    Returns:\n        np.ndarray: The resulting panorama image.\n    \"\"\"\n\n    # Read and store each image\n    pics = [cv2.imread(img_p).astype(np.float32) for img_p in img_paths]\n    n = len(pics)\n\n    # Initialize arrays to store original and transformed corner points\n    all_corners = np.empty((n, 4, 3))\n    for i in range(n):\n        # Define corner points for each image\n        all_corners[i] = [\n            [0, 0, 1],\n            [pics[i].shape[1], 0, 1],\n            [pics[i].shape[1], pics[i].shape[0], 1],\n            [0, pics[i].shape[0], 1],\n        ]\n\n    all_new_corners = np.empty((n, 4, 3))\n    for i in range(n):\n        # Apply homography transformations to each corner point\n        all_new_corners[i] = [\n            np.dot(transforms[i], corner) for corner in all_corners[i]\n        ]\n\n    # Reshape transformed corners for further processing\n    all_new_corners = all_new_corners.reshape(-3, 3)\n    x_news = all_new_corners[:, 0] / all_new_corners[:, 2]\n    y_news = all_new_corners[:, 1] / all_new_corners[:, 2]\n\n    # Determine min/max x and y coordinates for the panorama\n    y_min = min(y_news)\n    x_min = min(x_news)\n    y_max = int(round(max(y_news)))\n    x_max = int(round(max(x_news)))\n\n    # Calculate shifts to adjust the panorama's origin\n    x_shift = -min(x_min, 0)\n    y_shift = -min(y_min, 0)\n    T = np.array(\n        [[1, 0, x_shift], [0, 1, y_shift], [0, 0, 1]], dtype=\"float32\"\n    )\n\n    # Calculate new dimensions for the panorama\n    x_min = int(round(x_min))\n    y_min = int(round(y_min))\n    height_new = y_max - y_min\n    width_new = x_max - x_min\n    size = (width_new, height_new)\n\n    # Initialize the panorama using the first image\n    panorama_ans = cv2.warpPerspective(\n        src=pics[0],\n        M=T @ transforms[0],\n        dsize=size,\n        flags=cv2.INTER_NEAREST,\n        borderMode=cv2.BORDER_CONSTANT,\n        borderValue=(-1, -1, -1),\n    )\n\n    # Warp the remaining images into the panorama\n    for i in range(1, n):\n        cv2.warpPerspective(\n            pics[i],\n            T @ transforms[i],\n            size,\n            panorama_ans,\n            flags=cv2.INTER_NEAREST,\n            borderMode=cv2.BORDER_TRANSPARENT,\n        )\n\n    return panorama_ans\n\n\ndef vec_to_homography(vec: np.ndarray, i: int, pivot: int) -> np.ndarray:\n    \"\"\"Extract a 3x3 homography matrix from a flattened vector.\n\n    Args:\n        vec (np.ndarray): Flattened vector of all homographies\n            (except the pivot one, which is identical).\n\n        i (int): The index of the homography to be extracted.\n\n        pivot (int): The index of the pivot image.\n\n    Returns:\n        np.ndarray: The 3x3 homography matrix of the i-th image.\n    \"\"\"\n    # If the index is the pivot, return the identity matrix\n    if i == pivot:\n        return np.eye(3)\n    # Adjust index if it is greater than pivot\n    elif i > pivot:\n        i -= 1\n    # Extract the 3x3 homography matrix from the vector\n    H = vec[8 * i : 8 * (i + 1)]\n    H = np.array([[H[0], H[1], H[2]], [H[3], H[4], H[5]], [H[6], H[7], 1]])\n    return H\n\n\ndef homography_to_vec(Hs: List[np.ndarray], pivot: int) -> List[float]:\n    \"\"\"\n    Flatten a list of 3x3 homography matrices into a single vector.\n\n    Args:\n        Hs (List[np.ndarray]): A list of all homography matrices.\n        pivot (int): The index of the pivot image.\n\n    Returns:\n        List[float]: A flattened vector of all homography matrices\n        (except the pivot).\n    \"\"\"\n    n = len(Hs)\n    vec = np.empty(8 * (n - 1))\n    for i in range(n):\n        if i == pivot:\n            # Skip the pivot image\n            continue\n        elif i < pivot:\n            # The homography matrix is placed at the position of the image\n            H = Hs[i].reshape(-1)\n            H = H[:-1]  # Remove the last element (scale factor)\n            vec[8 * i : 8 * (i + 1)] = H\n        else:\n            # The homography matrix is placed at the position of the image\n            # minus one (since the pivot image is skipped)\n            H = Hs[i].reshape(-1)\n            H = H[:-1]  # Remove the last element (scale factor)\n            vec[8 * (i - 1) : 8 * i] = H\n    return vec\n\n\ndef dist(X: List[float], inliers: List[np.ndarray], pivot: int) -> np.ndarray:\n    \"\"\"\n    Calculate distances between the coordinates of all pairs of inliers\n    in the transformed coordinate system.\n\n    Args:\n        X (List[float]): Flattened vector of all homographies\n        (except the pivot one, which is identity).\n\n        inliers (List[np.ndarray]): List of inliers, each inlier is\n        an np.ndarray((i, j, x, y, xx, yy)), where i and j are the indices\n        of the images corresponding to the inlier, (x, y) are the coordinates\n        of the point on image i, and (xx, yy) are the coordinates of the\n        point on image j.\n\n        pivot (int): Index of the pivot image.\n\n    Returns:\n        np.ndarray: A vector of distances between the coordinates of all\n        pairs of inliers in the transformed coordinates.\n    \"\"\"\n    output = []  # Initialize the output list to store distances\n    for i, j, x, y, xx, yy in inliers:\n        # Get the homography matrices for images i and j\n        Hi = vec_to_homography(X, i, pivot)\n        Hj = vec_to_homography(X, j, pivot)\n\n        # Transform the coordinates using the homography matrices\n        first = np.dot(Hi, [x, y, 1])\n        first /= first[2]  # Normalize to get the final coordinates\n        second = np.dot(Hj, [xx, yy, 1])\n        second /= second[2]  # Normalize to get the final coordinates\n        output.append(first[0] - second[0])\n        output.append(first[1] - second[1])\n\n    return np.array(output)\n\n\ndef optimize(\n    Hs: List[np.ndarray],\n    inliers: List[np.ndarray],\n    pivot: int,\n) -> Tuple[List[np.ndarray], float, float]:\n    \"\"\"Global alignment using all inliers by adjusting all homography matrices.\n\n    Args:\n        Hs: a list of all homographies,\n\n        inliers: list of inliers, each inlier is\n        a np.ndarray((i, j, x, y, xx, yy)), where i and j are\n        the indices of the images corresponding to the inlier,\n        (x, y) are the coordinates of the point on image i,\n        and (xx, yy) are the coordinates of the point on image j\n\n        pivot: the number of the pivot image,\n\n    Returns:\n        a tuple of:\n            * a list of new homographies,\n            * the initial mean squared error,\n            * the optimized mean squared error,\n    \"\"\"\n    n = len(Hs)\n    vec = homography_to_vec(Hs, pivot)\n    norm = dist(vec, inliers, pivot)\n\n    init_error = (norm**2).mean() ** 0.5\n    res_lm = least_squares(\n        dist, vec, method=\"lm\", xtol=1e-6, ftol=1e-6, args=(inliers, pivot)\n    )\n    optim_error = (res_lm.fun**2).mean() ** 0.5\n    new_vec = res_lm.x\n\n    final_transforms = []\n    for i in range(n):\n        final_transforms.append(vec_to_homography(new_vec, i, pivot))\n    return final_transforms, init_error, optim_error\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:44:52.174439Z","iopub.execute_input":"2025-02-01T20:44:52.174889Z","iopub.status.idle":"2025-02-01T20:44:52.190654Z","shell.execute_reply.started":"2025-02-01T20:44:52.174864Z","shell.execute_reply":"2025-02-01T20:44:52.189727Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class Stitcher:\n    def __init__(self, device=None):\n        self.device = device if device else torch.device(\"cpu\")\n        self.matcher = KF.LoFTR(pretrained=\"outdoor\").to(self.device)\n        self.size = np.array((600, 400))\n\n    def _load_torch_tensors(\n        self, img_paths: Iterable[Path]\n    ) -> Tuple[List[Tuple[float]], List[torch.Tensor]]:\n        \"\"\"\n        Opens images from paths, saves the original dimensions,\n        converts the images to the required format, and collects\n        them into a list.\n\n        Args:\n            img_paths : paths to images\n\n        Returns:\n            A tuple of\n                - list of original sizes of images\n                - list of torch.Tensor of size [1, 1, 600, 400]\n        \"\"\"\n        images = []\n        orig_sizes = []\n        for path in img_paths:\n            img = Image.open(path).convert(\"L\")  # Convert to grayscale\n            orig_sizes.append(np.array(img.size))  # Save the original size\n            img = img.resize((600, 400), resample=Image.Resampling.LANCZOS)\n            img = torchvision.transforms.functional.pil_to_tensor(img)\n            img = img.unsqueeze(dim=0)\n            images.append(img)  # Append the processed tensor to the list\n\n        return orig_sizes, images\n\n    def stitch(\n        self,\n        img_paths: Iterable[Path],\n        verbose: bool = False,\n        logger: Union[bool, dict[str, list]] = False,\n    ) -> Image:\n        \"\"\"\n        A method that implements stitching a panorama from a collection\n        of image files.\n\n        Args:\n            verbose : If True, it outputs information about the execution time of the stages\n              and the accuracy of the stitching\n            logger : A technical argument used for logging information about execution time\n            and accuracy into a dictionary. Leave it as False\n        \"\"\"\n        start_time = time()\n        n = len(img_paths)\n        orig_sizes, images = self._load_torch_tensors(img_paths)\n\n        batch1 = []\n        batch2 = []\n        for i in range(n - 1):\n            for j in range(i + 1, n):\n                batch1.append(images[i])\n                batch2.append(images[j])\n\n        batch1 = torch.cat(batch1) / 255.0\n        batch2 = torch.cat(batch2) / 255.0\n\n        all_corr = []\n        batch_size = 10\n        total_infer = n * (n - 1) // 2\n        batch_num = (total_infer - 1) // batch_size + 1\n\n        if verbose:\n            print(f\"img processing done - {time() - start_time:.4}s\")\n        if logger:\n            logger[\"images num\"].append(n)\n            logger[\"preproc time\"].append(time() - start_time)\n\n        s_time = time()\n\n        # Run the LoFTR model on the images\n        for i in range(batch_num):\n            input_dict = {\n                \"image0\": batch1[batch_size * i: batch_size * (i + 1)].to(\n                    self.device\n                ),\n                \"image1\": batch2[batch_size * i: batch_size * (i + 1)].to(\n                    self.device\n                ),\n            }\n            with torch.inference_mode():\n                correspondences = self.matcher(input_dict)\n            tmp = {\n                \"batch_indexes\": correspondences[\"batch_indexes\"]\n                .detach()\n                .cpu(),\n                \"keypoints0\": correspondences[\"keypoints0\"].detach().cpu(),\n                \"keypoints1\": correspondences[\"keypoints1\"].detach().cpu(),\n                \"confidence\": correspondences[\"confidence\"].detach().cpu(),\n            }\n            all_corr.append(tmp)\n            del correspondences\n            torch.cuda.empty_cache()\n            gc.collect()\n\n        if verbose:\n            print(f\"LoFTR done - {time() - s_time:.4}s\")\n        if logger:\n            logger[\"LoFTR time\"].append(time() - s_time)\n        s_time = time()\n\n        # Filter out the correspondences with low confidence\n        inliers = []\n        diff_corr = []\n        for batch_corr in all_corr:\n            for i in range(batch_size):\n                idx = batch_corr[\"batch_indexes\"] == i\n                kp0 = batch_corr[\"keypoints0\"][idx]\n                kp1 = batch_corr[\"keypoints1\"][idx]\n                conf = batch_corr[\"confidence\"][idx]\n                kp0 *= orig_sizes[i] / self.size\n                kp1 *= orig_sizes[i] / self.size\n                diff_corr.append(\n                    np.concatenate([kp0, kp1, conf[..., None]], axis=-1)\n                )\n\n        good_corrs = []\n        for corrs in diff_corr:\n            corrs = corrs[corrs[:, 4] > 0.9]\n            good_corrs.append(corrs)\n\n        Hs = [[None] * n for _ in range(n)]\n        num_matches = np.zeros((n, n))\n        for i in range(n - 1):\n            for j in range(i + 1, n):\n                corrs = good_corrs.pop(0)\n                num = corrs.shape[0]\n                if num < 10:\n                    continue\n\n                num_matches[i][j] = num\n                num_matches[j][i] = num\n                Hs[i][j], mask_ij = cv2.findHomography(\n                    corrs[:, 0:2], corrs[:, 2:4], cv2.USAC_MAGSAC, 0.5\n                )\n                Hs[j][i], mask_ji = cv2.findHomography(\n                    corrs[:, 2:4], corrs[:, 0:2], cv2.USAC_MAGSAC, 0.5\n                )\n                inliers_ij = corrs[mask_ij.squeeze().astype(\"bool\")]\n\n                inli = inliers_ij[inliers_ij[:, -1].argsort()[::-1]][:15]\n                inli = inli[:, :-1]\n\n                inliers += [[i, j, *inl] for inl in inli]\n\n        if verbose:\n            print(f\"RANSAC done - {time() - s_time:.4}s\")\n        if logger:\n            logger[\"homography time\"].append(time() - s_time)\n\n        # Initialize the transformations for each image\n        transforms = [np.eye(3) for i in range(n)]\n\n        queryIdx = [i for i in range(n)]\n        Idx = [i for i in range(n)]\n        targetIdx = []\n\n        pivot = np.argmax(num_matches.sum(axis=1))\n        targetIdx.append(pivot)\n        queryIdx.remove(pivot)\n        Idx.remove(pivot)\n\n        while queryIdx:\n            a = num_matches[queryIdx, :][:, targetIdx]\n            curr, best_neighb = np.unravel_index(\n                np.argmax(a, axis=None), a.shape\n            )\n            H = (\n                transforms[targetIdx[best_neighb]]\n                @ Hs[Idx[curr]][targetIdx[best_neighb]]\n            )\n            H /= H[2, 2]\n            transforms[Idx[curr]] = H\n            targetIdx.append(Idx[curr])\n            queryIdx.remove(Idx[curr])\n            Idx.pop(curr)\n\n        s_time = time()\n        if logger:\n            logger[\"num inliers\"].append(len(inliers))\n\n        # Optimize the transformations\n        final_transforms, init_error, optim_error = optimize(\n            transforms, inliers, pivot\n        )\n\n        if verbose:\n            print(f\"optimization done - {time() - s_time:.4}s\")\n            print(f\"num inliers - {len(inliers)}\")\n            print(f\"initial error - {init_error:.4}\")\n            print(f\"optimized error - {optim_error:.4}s\")\n        if logger:\n            logger[\"optimization time\"].append(time() - s_time)\n            logger[\"initial error\"].append(init_error)\n            logger[\"optimized error\"].append(optim_error)\n        s_time = time()\n\n        \n        # Stitch the images\n        panorama_ans = transform_and_stitch(final_transforms, img_paths)\n\n        if verbose:\n            print(f\"stitching done - {time() - s_time:.4}s\")\n            print(f\"total_time - {time() - start_time:.4}s\")\n        if logger:\n            logger[\"stitching time\"].append(time() - s_time)\n            logger[\"total time\"].append(time() - start_time)\n\n        return Image.fromarray(panorama_ans.astype(\"uint8\"))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:44:52.272157Z","iopub.execute_input":"2025-02-01T20:44:52.272448Z","iopub.status.idle":"2025-02-01T20:44:52.291497Z","shell.execute_reply.started":"2025-02-01T20:44:52.272423Z","shell.execute_reply":"2025-02-01T20:44:52.290724Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"input_dir = Path('/kaggle/input/cwpna527-calibrated')\noutput_file = Path('output.jpg')\ndevice = 'cuda'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:44:52.292715Z","iopub.execute_input":"2025-02-01T20:44:52.293043Z","iopub.status.idle":"2025-02-01T20:44:52.309827Z","shell.execute_reply.started":"2025-02-01T20:44:52.292990Z","shell.execute_reply":"2025-02-01T20:44:52.309156Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"stchr = Stitcher(device=device)\n\nimg_paths = [\n    img_p\n    for img_p in input_dir.iterdir()\n    if img_p.suffix in (\".jpg\", \".png\")\n]\n\npanorama = stchr.stitch(img_paths=img_paths, verbose=True)\npanorama.save(output_file, quality=95)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:44:52.311474Z","iopub.execute_input":"2025-02-01T20:44:52.311772Z","iopub.status.idle":"2025-02-01T20:45:29.197864Z","shell.execute_reply.started":"2025-02-01T20:44:52.311752Z","shell.execute_reply":"2025-02-01T20:45:29.197124Z"}},"outputs":[{"name":"stdout","text":"img processing done - 2.555s\nLoFTR done - 20.24s\nRANSAC done - 0.6226s\noptimization done - 7.47s\nnum inliers - 686\ninitial error - 11.15\noptimized error - 0.4373s\nstitching done - 4.748s\ntotal_time - 35.64s\n","output_type":"stream"}],"execution_count":11}]}